{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save protein sequences as a FASTA file as required by the embedding generator\n",
    "import pickle5 as pickle\n",
    "from tqdm import tqdm\n",
    "# Load pickle file\n",
    "with open('data/datasetB_all_sequences.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Write protein sequences in a fasta file\n",
    "with open(\"data/datasetB_all_sequences.fasta\", \"w\") as f:\n",
    "    for key in tqdm(data):\n",
    "        f.write('>'+key+'\\n')\n",
    "        f.write(data[key])\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1g58_A 196\n",
      "torch.Size([196, 768])\n",
      "tensor([[-0.7342,  0.7721, -0.2252,  ..., -0.7896,  1.2874,  0.9320],\n",
      "        [-0.0884,  0.7979, -0.2805,  ...,  0.1627,  0.5936,  0.3086],\n",
      "        [ 0.7058, -0.5040, -0.4714,  ..., -0.1014,  1.6905, -0.0378],\n",
      "        ...,\n",
      "        [ 0.0742, -0.1984,  0.2745,  ..., -0.9772,  1.1318, -0.0934],\n",
      "        [-0.0326,  0.4182, -0.3380,  ..., -1.2327, -0.1898,  1.3206],\n",
      "        [-0.5076, -0.3772,  0.8009,  ..., -1.1297,  0.0080, -0.9833]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models = ['esm2_t33_650M_UR50D', 'esm2_t36_3B_UR50D', 'esm1_t34_670M_UR50S',\n",
    "        'esm1_t6_43M_UR50S', 'esm2_t30_150M_UR50D', 'esm1v_t33_650M_UR90S_1',\n",
    "        'esm1b_t33_650M_UR50S', 'esm2_t12_35M_UR50D', 'esm_msa1b_t12_100M_UR50S',\n",
    "        'esm1_t34_670M_UR50D', 'esm_msa1_t12_100M_UR50S', 'esm2_t6_8M_UR50D',\n",
    "        'esm_if1_gvp4_t16_142M_UR50']\n",
    "\n",
    "import os, torch\n",
    "# Process extracted emb\n",
    "from Bio import SeqIO\n",
    "fasta_sequences = SeqIO.parse(open('data/datasetB_all_sequences.fasta'),'fasta')\n",
    "\n",
    "model_name='esm1_t6_43M_UR50S'\n",
    "\n",
    "for fasta in fasta_sequences:\n",
    "    #print('Filename:', datapoint)\n",
    "    print(fasta.id, len(fasta.seq))\n",
    "    emb = torch.load(model_name+'/'+fasta.id+'.pt')\n",
    "    key = list(emb['representations'].keys())\n",
    "    print(emb['representations'][key[0]].shape)\n",
    "    print(emb['representations'][key[0]])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch, os\n",
    "concatenated_dataset = []\n",
    "model_name='esm1_t6_43M_UR50S'\n",
    "#for fasta in tqdm(fasta_sequences):\n",
    "i = 0\n",
    "for datapoint in tqdm(os.listdir(model_name)):\n",
    "    #emb = torch.load(model_name+'/'+fasta.id+'.pt')\n",
    "    emb = torch.load(model_name+'/'+datapoint)['representations'][6]\n",
    "    concatenated_dataset.append(emb)\n",
    "    \n",
    "final_dataset = torch.cat(concatenated_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([451563, 768]), torch.Size([48, 768]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(concatenated_dataset).shape, emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ProtEmbDataset(Dataset):\n",
    "    \"\"\"Protein emb dataset.\"\"\"\n",
    "    def __init__(self, model_name, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the datapoints.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.filenames = os.listdir(model_name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.filenames[idx]\n",
    "        emb = torch.load(model_name+'/'+file)\n",
    "        key = list(emb['representations'].keys())\n",
    "        sample = emb['representations'][key[0]]\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'esm1_t6_43M_UR50S'\n",
    "root_dir = 'data'\n",
    "protein_dataset = ProtEmbDataset(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(protein_dataset, batch_size=1,\n",
    "                        shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n",
      "torch.Size([1, 162, 768])\n"
     ]
    }
   ],
   "source": [
    "for datapoint in dataloader:\n",
    "    #datapoint = datapoint.view(-1)\n",
    "    print(torch.squeeze(datapoint).shape)\n",
    "    print(datapoint.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "\n",
    "# Create a validation and training set\n",
    "samples_count = len(protein_dataset)\n",
    "all_samples_indexes = list(range(samples_count))\n",
    "np.random.shuffle(all_samples_indexes)\n",
    "\n",
    "al_ratio = 0.2\n",
    "val_end = int(samples_count * 0.2)\n",
    "val_indexes = all_samples_indexes[0:val_end]\n",
    "train_indexes = all_samples_indexes[val_end:]\n",
    "assert len(val_indexes) + len(train_indexes) == samples_count , 'the split is not valid' \n",
    "\n",
    "sampler_train = data.SubsetRandomSampler(train_indexes)\n",
    "sampler_val = data.SubsetRandomSampler(val_indexes)\n",
    "\n",
    "dataloader_train = DataLoader(protein_dataset, batch_size=1, sampler = sampler_train, num_workers=4)\n",
    "dataloader_val = DataLoader(protein_dataset, batch_size=1, sampler = sampler_val, num_workers=4)\n",
    "# dataloader_test = DataLoader(protein_dataset, batch_size=1,\n",
    "#                         shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "class IonNet(nn.Module):\n",
    "    def __init__(self, d_in):\n",
    "        super(IonNet, self).__init__()\n",
    "        #self.net = models.resnet18(pretrained=True)\n",
    "        self.n_features = 256\n",
    "        self.linear1 = nn.Linear(d_in, 256) \n",
    "        #self.linear2 = nn.Linear(512, 128)\n",
    "        self.fc = nn.Identity()\n",
    "        self.fc1 = nn.Sequential(OrderedDict([('linear', nn.Linear(self.n_features,self.n_features)),('relu1', nn.ReLU()),('final', nn.Linear(self.n_features, 1))]))   #Ion1\n",
    "        self.fc2 = nn.Sequential(OrderedDict([('linear', nn.Linear(self.n_features,self.n_features)),('relu1', nn.ReLU()),('final', nn.Linear(self.n_features, 1))]))   #Ion2\n",
    "        self.fc3 = nn.Sequential(OrderedDict([('linear', nn.Linear(self.n_features,self.n_features)),('relu1', nn.ReLU()),('final', nn.Linear(self.n_features, 1))]))   #Ion3\n",
    "        self.fc4 = nn.Sequential(OrderedDict([('linear', nn.Linear(self.n_features,self.n_features)),('relu1', nn.ReLU()),('final', nn.Linear(self.n_features, 1))]))   #Null\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ion1_head = self.fc1(self.linear1(x))\n",
    "        ion2_head = self.fc2(self.linear1(x))\n",
    "        ion3_head = self.fc3(self.linear1(x))\n",
    "        null_head = self.fc4(self.linear1(x))\n",
    "        return ion1_head, ion2_head, ion3_head, null_head\n",
    "\n",
    "import torch.nn\n",
    "class CNN2Layers(torch.nn.Module):\n",
    "    def __init__(self, in_channels, n_features, kernel_size, stride, padding, dropout):\n",
    "        super(CNN2Layers, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.conv1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels=in_channels, out_channels=512, kernel_size=kernel_size,\n",
    "                            stride=stride, padding=padding),\n",
    "            torch.nn.ELU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "\n",
    "            torch.nn.Conv1d(in_channels=512, out_channels=self.n_features, kernel_size=kernel_size,\n",
    "                            stride=stride, padding=padding),\n",
    "        )\n",
    "\n",
    "        self.linear1 = nn.Linear(self.n_features, self.n_features)\n",
    "\n",
    "        self.fc1 = nn.Sequential(OrderedDict([('linear', nn.Linear(self.n_features,self.n_features)),('relu1', nn.ReLU()),('final', nn.Linear(self.n_features, 1))]))   #Ion1\n",
    "        self.fc2 = nn.Sequential(OrderedDict([('linear', nn.Linear(self.n_features,self.n_features)),('relu1', nn.ReLU()),('final', nn.Linear(self.n_features, 1))]))   #Ion2\n",
    "        self.fc3 = nn.Sequential(OrderedDict([('linear', nn.Linear(self.n_features,self.n_features)),('relu1', nn.ReLU()),('final', nn.Linear(self.n_features, 1))]))   #Ion3\n",
    "        self.fc4 = nn.Sequential(OrderedDict([('linear', nn.Linear(self.n_features,self.n_features)),('relu1', nn.ReLU()),('final', nn.Linear(self.n_features, 1))]))  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        #print(x.shape)\n",
    "        ion1_head = self.fc1(self.linear1(x))\n",
    "        ion2_head = self.fc2(self.linear1(x))\n",
    "        ion3_head = self.fc3(self.linear1(x))\n",
    "        null_head = self.fc4(self.linear1(x))\n",
    "        return ion1_head, ion2_head, ion3_head, null_head\n",
    "\n",
    "# RuntimeError: Given groups=1, weight of size [512, 768, 3], expected input[1, 674, 768] to have 768 channels, but got 674 channels instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING MODEL\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CNN2Layers(in_channels=768, n_features=256, kernel_size=1, stride=1, padding=0, dropout=0.7).to(device)\n",
    "#a = model(torch.unsqueeze(dataloader.dataset[0][0],(2)))\n",
    "#torch.unsqueeze(dataloader.dataset[0][0], (2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = list(dataloader.dataset[0][1].keys())\n",
    "accuracies = [0.0]*len(fields)\n",
    "#status = 'training' if is_training else 'validation'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for i, (embs, labels) in tqdm(enumerate(dataloader)):\n",
    "    embs = torch.squeeze(embs)\n",
    "    embs = torch.unsqueeze(embs, (2)).to(device)\n",
    "    #print(fields)\n",
    "    labels = [torch.tensor(labels[key], dtype=float).to(device) for key in fields]\n",
    "    (lbl_ion1, lbl_ion2, lbl_ion3, lbl_null) = labels\n",
    "    preds = model(embs)\n",
    "    (prd_ion1, prd_ion2, prd_ion3, prd_null) = preds\n",
    "    \n",
    "    #accuracies = [0.0]*len(fields)\n",
    "    accuracies[0] = torch.mean((torch.round(prd_ion1) == lbl_ion1).float())\n",
    "    accuracies[1] = torch.mean((torch.round(prd_ion2) == lbl_ion2).float())\n",
    "    accuracies[2] = torch.mean((torch.round(prd_ion3) == lbl_ion3).float())\n",
    "    accuracies[3] = torch.mean((torch.round(prd_null) == lbl_null).float())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ((lbl_ion1 == 1).nonzero(as_tuple=True)[0])\n",
    "print ((torch.round(prd_ion1) == 1).nonzero(as_tuple=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(model, dataloader, optimizer, criterion_1, is_training, device, topk, interval):\n",
    "    batch_cnt = len(dataloader)\n",
    "    fields = list(dataloader.dataset[0][1].keys())\n",
    "    accuracies = [0.0]*len(fields)\n",
    "    status = 'training' if is_training else 'validation'\n",
    "\n",
    "    with torch.set_grad_enabled(is_training):\n",
    "        model.train() if is_training else model.eval()\n",
    "\n",
    "        for i, (embs, labels) in enumerate(dataloader):\n",
    "            embs = torch.squeeze(embs)\n",
    "            embs = torch.unsqueeze(embs, (2)).to(device)\n",
    "            #print(fields)\n",
    "            labels = [torch.tensor(labels[key], dtype=float).to(device) for key in fields]\n",
    "            (lbl_ion1, lbl_ion2, lbl_ion3, lbl_null) = labels\n",
    "            preds = model(embs)\n",
    "            (prd_ion1, prd_ion2, prd_ion3, prd_null) = preds\n",
    "\n",
    "            #print(torch.round(torch.squeeze(prd_ion1)).shape, lbl_ion1)\n",
    "            loss_ion1 = criterion_1(torch.squeeze(prd_ion1), lbl_ion1)\n",
    "            loss_ion2 = criterion_1(torch.squeeze(prd_ion2), lbl_ion2)\n",
    "            loss_ion3 = criterion_1(torch.squeeze(prd_ion3), lbl_ion3)\n",
    "            loss_null = criterion_1(torch.squeeze(prd_null), lbl_null)\n",
    "\n",
    "            loss_final = loss_ion1 + loss_ion2 + loss_ion3 + loss_null\n",
    "\n",
    "            #print(torch.squeeze(prd_ion1))\n",
    "            # # accuracies \n",
    "            # _, indxs_ion1 = torch.squeeze(prd_ion1).topk(topk)\n",
    "            # print(indxs_ion1)\n",
    "            # _, indxs_ion2 = torch.squeeze(prd_ion2).topk(topk,dim=1)\n",
    "            # _, indxs_ion3 = torch.squeeze(prd_ion3).topk(topk,dim=1)\n",
    "            # _, indxs_null = torch.squeeze(prd_null).topk(topk,dim=1)\n",
    "\n",
    "            #print(indxs_ion1)\n",
    "            #accuracies = [0.0]*len(fields)\n",
    "            accuracies[0] += torch.mean((torch.round(prd_ion1) == lbl_ion1).float())\n",
    "            accuracies[1] += torch.mean((torch.round(prd_ion2) == lbl_ion2).float())\n",
    "            accuracies[2] += torch.mean((torch.round(prd_ion3) == lbl_ion3).float())\n",
    "            accuracies[3] += torch.mean((torch.round(prd_null) == lbl_null).float())\n",
    "\n",
    "            #print(accuracies)\n",
    "\n",
    "            if is_training:\n",
    "                optimizer.zero_grad()\n",
    "                loss_final.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "            if i%interval==0:\n",
    "                accs = [acc/batch_cnt for acc in accuracies]\n",
    "                print(f'[{status}] iter: {i} loss: {loss_final.item():6f}')\n",
    "                print (' ,'.join(list(f'{f}: {x:.4f}' for f, x in zip(fields, accs))))\n",
    "\n",
    "def train_loop(model, epochs, dataloader_train, dataloader_val,\n",
    "               optimizer, lr_scheduler, criterion_1, criterion_2, interval=10):\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    for e in range(epochs):\n",
    "        lrs = [f'{lr:.6f}' for lr in lr_scheduler.get_lr()]\n",
    "        print(f'epoch {e} : lrs : {\" \".join(lrs)}')\n",
    "        train_val(model, dataloader_train, optimizer, criterion_1, True, device, 1, interval)\n",
    "        train_val(model, dataloader_val, optimizer, criterion_1, False, device, 1, 1)\n",
    "        lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# #device = 'cpu'\n",
    "#model = IonNet(768).to(device=device)\n",
    "model = CNN2Layers(in_channels=768, n_features=512, kernel_size=3, stride=1, padding=1, dropout=0.7).to(device=device)\n",
    "criterion_1 = nn.BCEWithLogitsLoss()\n",
    "#criterion_1 = nn.MSELoss()\n",
    "epochs = 10\n",
    "lr = 0.0001\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
    "\n",
    "# optimizer.add_param_group({\"params\": model.fc_1.parameters(), \"lr\": 0.1})\n",
    "# optimizer.add_param_group({\"params\": model.fc_2.parameters(), \"lr\": 0.1})\n",
    "# optimizer.add_param_group({\"params\": model.fc_3.parameters(), \"lr\": 0.1})\n",
    "# optimizer.add_param_group({\"params\": model.fc_4.parameters(), \"lr\": 0.1})\n",
    "\n",
    "lrsched = torch.optim.lr_scheduler.StepLR(optimizer, 10)\n",
    "\n",
    "train_loop(model, epochs, dataloader_train, dataloader_val, optimizer, lrsched, criterion_1, 5)\n",
    "\n",
    "# ion1_loss = nn.MSELoss() # Includes Softmax\n",
    "# ion2_loss = nn.MSELoss() # Doesn't include Softmax\n",
    "# ion3_loss = nn.MSELoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.09)\n",
    "# sig = nn.Sigmoid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('ion')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9eea4ac00fc0dad156dbd79745452f299f7f980b7a63c4943f7dd0031d0ee12d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
